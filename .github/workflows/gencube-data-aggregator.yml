name: Gencube Data Aggregator Pipeline

on:
  schedule:
    # Runs once daily at 00:00 KST (15:00 UTC)
    - cron: '0 15 * * *'
  workflow_dispatch:

jobs:
  data-aggregator:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          persist-credentials: false

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y lftp rsync
          pip install --upgrade pip
          pip install selenium

      - name: Create data-aggregator directories
        run: |
          mkdir -p data-aggregator/ensembl-beta_ftp_metadata

      - name: Fetch FTP metadata using lftp mirror (dry-run + parallel)
        env:
          FTP_SERVER: "ftp.ebi.ac.uk"
          FTP_PATH: "/pub/ensemblorganisms"
        run: |
          lftp -c "
            open $FTP_SERVER
            set ftp:prefer-epsv true
            set net:socket-buffer 33554432
            mirror --verbose --parallel=10 --dry-run --no-perms \
              $FTP_PATH data-aggregator/ensembl-beta_ftp_metadata
          " > data-aggregator/ensembl-beta_ftp_listing.txt

      - name: Convert mirror listing to JSON (excluding 'vep' and 'test')
        run: |
          python3 << 'EOF'
          import os, json

          def build_from_listing(listing_file, root_dir):
              tree = {}
              with open(listing_file) as f:
                  for line in f:
                      line = line.strip()
                      if line.startswith('mkdir '):
                          local = line.split(' ',1)[1]
                          rel = os.path.relpath(local, root_dir)
                          if rel in ('.', ''): continue
                          parts = rel.split(os.sep)
                          node = tree
                          for p in parts:
                              node = node.setdefault(p, {})
                      elif line.startswith('get ') and '->' in line:
                          local = line.split('->',1)[1].strip()
                          rel = os.path.relpath(local, root_dir)
                          parts = rel.split(os.sep)
                          node = tree
                          for p in parts[:-1]:
                              node = node.setdefault(p, {})
                          node[parts[-1]] = None
              return tree

          def remove_unwanted(tree):
              tree.pop("test", None)
              for species, genomes in list(tree.items()):
                  if isinstance(genomes, dict):
                      for gid, contents in list(genomes.items()):
                          if gid.startswith(("GCA_","GCF_")) and isinstance(contents, dict):
                              contents.pop("vep", None)
              return tree

          if __name__ == "__main__":
              listing = "data-aggregator/ensembl-beta_ftp_listing.txt"
              root = "data-aggregator/ensembl-beta_ftp_metadata"
              tree = build_from_listing(listing, root)
              tree = remove_unwanted(tree)
              out_path = "data-aggregator/ensembl-beta_ftp_structure.json"
              with open(out_path, "w") as f:
                  json.dump(tree, f, indent=4)
              print(f"Directory structure saved to {out_path}")
          EOF

      - name: Generate information files
        run: |
          python3 << 'EOF'
          import json

          with open("data-aggregator/ensembl-beta_ftp_structure.json") as f:
              data = json.load(f)

          species_lines = ["Species\tGenome"]
          total_lines = ["Species\tGenome"]
          species_count = 0
          genome_count = 0

          for species, genomes in data.items():
              count = sum(1 for gid in genomes.keys() if gid.startswith(("GCA_","GCF_")))
              species_lines.append(f"{species}\t{count}")
              species_count += 1
              genome_count += count

          total_lines.append(f"{species_count}\t{genome_count}")

          with open("data-aggregator/ensembl-beta_count_species.txt", "w") as f:
              f.write("\n".join(species_lines))
          with open("data-aggregator/ensembl-beta_count_total.txt", "w") as f:
              f.write("\n".join(total_lines))

          print("Information files saved.")
          EOF

      - name: Fetch SRA advanced index list using Selenium and save as JSON
        run: |
          python3 << 'EOF'
          import time, re, json
          from selenium import webdriver
          from selenium.webdriver.common.by import By
          from selenium.webdriver.support.ui import WebDriverWait, Select
          from selenium.webdriver.support import expected_conditions as EC

          options = webdriver.ChromeOptions()
          options.add_argument("--headless")

          driver = webdriver.Chrome(options=options)
          driver.get("https://www.ncbi.nlm.nih.gov/sra/advanced")
          time.sleep(3)

          fields = [
              ("Strategy", "LS_STRATEGY"),
              ("Source", "LS_SOURCE"),
              ("Platform", "LS_PLATFORM"),
              ("Selection", "LS_SELECTION"),
              ("Filter", "LS_FILTER"),
              ("Properties", "LS_PROPERTIES")
          ]

          results = {}
          for label, var in fields:
              Select(driver.find_element(By.ID, "ff_0")).select_by_visible_text(label)
              time.sleep(1)
              btn = WebDriverWait(driver, 10).until(
                  EC.element_to_be_clickable((By.XPATH, "//select[@id='ff_0']/following::a[@class='show_index'][1]"))
              )
              driver.execute_script("arguments[0].scrollIntoView(true);", btn)
              time.sleep(1)
              driver.execute_script("arguments[0].click();", btn)
              time.sleep(3)

              opts = driver.find_element(By.ID, "terms_list").find_elements(By.TAG_NAME, "option")
              processed = []
              for o in opts:
                  text = o.text.strip()
                  text = re.sub(r'\s*\(.*?\)', '', text)
                  processed.append(text.replace(' ', '_'))
              results[var] = processed

          driver.quit()
          with open("data-aggregator/sra_advanced_index.json", "w") as f:
              json.dump(results, f, indent=4)
          print("SRA advanced index list saved.")
          EOF

      - name: Commit and push changes
        run: |
          git config --global user.name "keun-hong"
          git config --global user.email "thsrms9216@gmail.com"
          if [ -n "$(git status --porcelain data-aggregator/)" ]; then
            git add data-aggregator/
            git commit -m "Update FTP metadata structure, info counts, and SRA index"
            git push https://snu-cdrc:${{ secrets.MY_PAT }}@github.com/snu-cdrc/gencube.git
          else
            echo "No changes to commit."
          fi
