name: Gencube Data Aggregator Pipeline

on:
  schedule:
    # Runs once daily at 00:00 KST (15:00 UTC)
    - cron: '0 15 * * *'
  workflow_dispatch:

jobs:
  data-aggregator:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          persist-credentials: false

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y lftp
          pip install --upgrade pip
          pip install selenium

      - name: Create data-aggregator folder
        run: mkdir -p data-aggregator

      - name: Fetch FTP folder structure (NLST 방식)
        env:
          FTP_SERVER: "ftp.ebi.ac.uk"
          FTP_PATH: "/pub/ensemblorganisms"
        run: |
          lftp -c "
            set ftp:list-options -a;
            set dns:order 'inet inet6';
            open $FTP_SERVER;
            cd $FTP_PATH;
            cls --list-only -R
          " > data-aggregator/ensembl-beta_ftp_structure.txt

      - name: Convert structure to JSON (excluding 'vep' folders and the 'test' folder)
        run: |
          python3 << 'EOF'
          import json

          def parse_ls_output(file_path):
              tree = {}
              current_path = []
              with open(file_path, "r") as f:
                  for line in f:
                      line = line.rstrip("\n")
                      if not line or line == "$FTP_PATH:":
                          continue
                      if line.endswith(":"):
                          header = line[:-1]
                          if header.startswith("./"):
                              header = header[2:]
                          current_path = [] if header == "." else header.split("/")
                          continue
                      filename = line.rstrip("/")
                      is_dir = line.endswith("/")
                      node = tree
                      for part in current_path:
                          node = node.setdefault(part, {})
                      node[filename] = {} if is_dir else None
              return tree

          def remove_unwanted(tree):
              tree.pop("test", None)
              for species, genomes in tree.items():
                  if isinstance(genomes, dict):
                      for g in list(genomes):
                          if g.startswith(("GCA_", "GCF_")):
                              genomes[g].pop("vep", None)
              return tree

          if __name__ == "__main__":
              inp = "data-aggregator/ensembl-beta_ftp_structure.txt"
              out = "data-aggregator/ensembl-beta_ftp_structure.json"
              tree = remove_unwanted(parse_ls_output(inp))
              with open(out, "w") as f:
                  json.dump(tree, f, indent=4)
              print(f"Saved JSON to {out}")
          EOF

      - name: Generate information file
        run: |
          python3 << 'EOF'
          import json

          with open("data-aggregator/ensembl-beta_ftp_structure.json") as f:
              data = json.load(f)

          species_list = ["Species\tGenome"]
          total_list = ["Species\tGenome"]
          sp_cnt = 0
          gn_cnt = 0

          for sp, genomes in data.items():
              if sp == "test": continue
              cnt = sum(1 for g in genomes if g.startswith(("GCA_", "GCF_")))
              species_list.append(f"{sp}\t{cnt}")
              sp_cnt += 1
              gn_cnt += cnt

          total_list.append(f"{sp_cnt}\t{gn_cnt}")

          with open("data-aggregator/ensembl-beta_count_species.txt", "w") as f:
              f.write("\n".join(species_list))
          with open("data-aggregator/ensembl-beta_count_total.txt", "w") as f:
              f.write("\n".join(total_list))

          print("Count files written.")
          EOF

      - name: Fetch SRA advanced index list using Selenium and save as JSON
        run: |
          python3 << 'EOF'
          import time, re, json
          from selenium import webdriver
          from selenium.webdriver.common.by import By
          from selenium.webdriver.support.ui import WebDriverWait, Select
          from selenium.webdriver.support import expected_conditions as EC

          opts = webdriver.ChromeOptions()
          opts.add_argument("--headless")
          driver = webdriver.Chrome(options=opts)
          driver.get("https://www.ncbi.nlm.nih.gov/sra/advanced")
          time.sleep(3)

          fields = [
            ("Strategy", "LS_STRATEGY"),
            ("Source", "LS_SOURCE"),
            ("Platform", "LS_PLATFORM"),
            ("Selection", "LS_SELECTION"),
            ("Filter", "LS_FILTER"),
            ("Properties", "LS_PROPERTIES")
          ]
          out = {}
          for label, var in fields:
              Select(driver.find_element(By.ID, "ff_0")).select_by_visible_text(label)
              time.sleep(1)
              btn = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.XPATH, "//select[@id='ff_0']/following::a[@class='show_index'][1]"))
              )
              driver.execute_script("arguments[0].scrollIntoView();", btn)
              time.sleep(1)
              driver.execute_script("arguments[0].click();", btn)
              time.sleep(2)
              opts_e = driver.find_element(By.ID, "terms_list").find_elements(By.TAG_NAME, "option")
              lst = []
              for o in opts_e:
                  txt = re.sub(r'\s*\(.*?\)', "", o.text).replace(" ", "_")
                  lst.append(txt)
              out[var] = lst

          driver.quit()
          with open("data-aggregator/sra_advanced_index.json", "w") as f:
              json.dump(out, f, indent=4)
          print("SRA index JSON saved.")
          EOF

      - name: Commit and push changes
        run: |
          git config --global user.name "keun-hong"
          git config --global user.email "thsrms9216@gmail.com"
          if [ -n "$(git status --porcelain data-aggregator/ensembl-beta_ftp_structure.json data-aggregator/ensembl-beta_count_species.txt data-aggregator/ensembl-beta_count_total.txt data-aggregator/sra_advanced_index.json)" ]; then
            git add data-aggregator/*.json data-aggregator/*.txt
            git commit -m "Daily update: FTP 구조 & 통계 & SRA 인덱스"
            git push https://snu-cdrc:${{ secrets.MY_PAT }}@github.com/snu-cdrc/gencube.git
          else
            echo "No changes."
          fi
