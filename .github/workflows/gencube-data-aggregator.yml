name: Gencube Data Aggregator Pipeline

on:
  schedule:
    - cron: '0 15 * * *'  # Runs daily at 00:00 KST (15:00 UTC)
  workflow_dispatch:

jobs:
  data-aggregator:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          persist-credentials: false

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y lftp
          pip install --upgrade pip
          pip install selenium pandas requests

      - name: Create data-aggregator folder
        run: mkdir -p data-aggregator

      - name: Fetch FTP folder structure using ls -R (timeout after 10 minutes)
        env:
          FTP_SERVER: "ftp.ebi.ac.uk"
          FTP_PATH: "/pub/ensemblorganisms"
        run: |
          timeout 10m lftp -c "open $FTP_SERVER; cd $FTP_PATH; ls -R" \
            > data-aggregator/ensembl-beta_ftp_structure.txt || \
          { echo 'lftp command timed out after 10 minutes'; exit 1; }

      - name: Convert structure to JSON (drop only root-level files, remove 'test' and 'vep' directories)
        run: |
          python3 << 'EOF'
          import json

          def parse_ls_output(fp):
              """
              Parse recursive directory listing into nested dict.
              Keys are file or directory names; directories have dict values, files have None.
              """
              tree = {}
              path_stack = []
              with open(fp) as f:
                  for line in f:
                      line = line.rstrip("\n")
                      if not line:
                          continue
                      if line.endswith(":"):
                          header = line[:-1]
                          if header in (".", "./"):
                              path_stack = []
                          else:
                              path_stack = header.lstrip("./").split("/")
                          continue
                      parts = line.split()
                      if len(parts) < 9:
                          continue
                      name = " ".join(parts[8:])
                      is_dir = parts[0].startswith("d")
                      node = tree
                      for part in path_stack:
                          node = node.setdefault(part, {})
                      node[name] = {} if is_dir else None
              return tree

          def prune_root_files(tree):
              """
              Remove only top-level file entries (value=None).
              """
              for key in list(tree.keys()):
                  if tree[key] is None:
                      del tree[key]

          def remove_unwanted(tree):
              """
              Remove 'test' at top level and 'vep' under each genome directory.
              """
              tree.pop("test", None)
              for species, genomes in list(tree.items()):
                  if not isinstance(genomes, dict):
                      continue
                  for genome_id, contents in list(genomes.items()):
                      if isinstance(contents, dict):
                          contents.pop("vep", None)
              return tree

          tree = parse_ls_output("data-aggregator/ensembl-beta_ftp_structure.txt")
          prune_root_files(tree)
          tree = remove_unwanted(tree)
          with open("data-aggregator/ensembl-beta_ftp_structure.json", "w") as out:
              json.dump(tree, out, indent=4)
          print("Saved JSON with root-level files pruned.")
          EOF

      - name: Generate information files
        run: |
          python3 << 'EOF'
          import json

          # Load JSON structure
          with open("data-aggregator/ensembl-beta_ftp_structure.json") as f:
              data = json.load(f)

          # Prepare count lines: species vs number of assemblies
          species_lines = ["Species\tGenomeCount"]
          total_lines = ["TotalSpecies\tTotalGenomes"]
          species_count = 0
          genome_count = 0

          for species, genomes in data.items():
              if not isinstance(genomes, dict):
                  continue
              # Count assemblies starting with GCA_ or GCF_
              count = sum(1 for gid in genomes if gid.startswith(("GCA_","GCF_")))
              species_lines.append(f"{species}\t{count}")
              species_count += 1
              genome_count += count

          total_lines.append(f"{species_count}\t{genome_count}")

          # Save counts
          with open("data-aggregator/ensembl-beta_count_species.txt", "w") as sf:
              sf.write("\n".join(species_lines))
          with open("data-aggregator/ensembl-beta_count_total.txt", "w") as tf:
              tf.write("\n".join(total_lines))

          print("Saved species and genome count files.")
          EOF

      - name: Fetch Zoonomia metadata and save as pickle
        run: |
          python3 << 'EOF'
          import pandas as pd
          import requests
          from io import StringIO
          import pickle

          ZOONOMIA_URL = 'https://genome.senckenberg.de/download/TOGA'
          DIC_ZOONOMIA = {
              'human':                'human_hg38_reference',
              'mouse':                'mouse_mm10_reference',
              'chicken':              'chicken_galGal6_reference',
              'greenSeaturtle':       'greenSeaturtle_HLcheMyd2_reference',
              'pikePerch':            'pikePerch_HLsanLuc1_reference',
              'purpleSeaUrchin':      'purpleSeaUrchin_HLstrPur5_reference',
              'redEaredSlideTurtle':  'redEaredSlideTurtle_HLtraScrEle1_reference',
              'thaleCress':           'thaleCress_HLParaTha1_reference',
              'tobaccoHawkmoth':      'tobaccoHawkmoth_HLmanSex2_reference'
          }

          frames = []
          for ref_name, subdir in DIC_ZOONOMIA.items():
              url = f"{ZOONOMIA_URL}/{subdir}/overview.table.tsv"
              response = requests.get(url, verify=False)
              response.raise_for_status()
              csv_io = StringIO(response.text)
              df = pd.read_csv(csv_io, sep='\t')
              df['reference'] = ref_name
              frames.append(df)

          df_zoonomia = pd.concat(frames, ignore_index=True)

          # Save as pickle with protocol 4 for numpy<2 compatibility
          with open("data-aggregator/zoonomia_meta.pkl", "wb") as f:
              pickle.Pickler(f, protocol=4, fix_imports=True).dump(df_zoonomia)

          print("Saved Zoonomia metadata as pickle (protocol=4).")
          EOF

      - name: Fetch SRA advanced index list using Selenium and save as JSON
        run: |
          python3 << 'EOF'
          import re
          import json
          import time
          from selenium import webdriver
          from selenium.webdriver.common.by import By
          from selenium.webdriver.support.ui import WebDriverWait, Select
          from selenium.webdriver.support import expected_conditions as EC

          # Initialize headless browser
          options = webdriver.ChromeOptions()
          options.add_argument("--headless")
          driver = webdriver.Chrome(options=options)

          driver.get("https://www.ncbi.nlm.nih.gov/sra/advanced")
          time.sleep(3)

          fields = [
              ("Strategy",   "LS_STRATEGY"),
              ("Source",     "LS_SOURCE"),
              ("Platform",   "LS_PLATFORM"),
              ("Selection",  "LS_SELECTION"),
              ("Filter",     "LS_FILTER"),
              ("Properties", "LS_PROPERTIES"),
          ]

          results = {}
          for label, var in fields:
              Select(driver.find_element(By.ID, "ff_0")).select_by_visible_text(label)
              time.sleep(1)
              btn = WebDriverWait(driver, 10).until(
                  EC.element_to_be_clickable((By.XPATH, "//select[@id='ff_0']/following::a[@class='show_index'][1]"))
              )
              btn.click()
              time.sleep(2)
              opts = driver.find_element(By.ID, "terms_list").find_elements(By.TAG_NAME, "option")
              values = [re.sub(r'\s*\(.*?\)', "", opt.text).strip().replace(" ", "_") for opt in opts]
              results[var] = values

          driver.quit()

          with open("data-aggregator/sra_advanced_index.json", "w") as out:
              json.dump(results, out, indent=4)
          print("Saved SRA advanced index JSON.")
          EOF

      - name: Commit and push changes
        run: |
          git config --global user.name "keun-hong"
          git config --global user.email "thsrms9216@gmail.com"
          if [ -n "$(git status --porcelain data-aggregator/)" ]; then
            git add data-aggregator/
            git commit -m "Update FTP structure, counts, Zoonomia metadata & SRA index"
            git push https://snu-cdrc:${{ secrets.MY_PAT }}@github.com/snu-cdrc/gencube.git
          else
            echo "No changes to commit."
          fi
