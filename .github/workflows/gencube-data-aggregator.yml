name: Gencube Data Aggregator Pipeline

on:
  schedule:
    # Runs once daily at 00:00 KST (15:00 UTC)
    - cron: '0 15 * * *'
  workflow_dispatch:

jobs:
  data-aggregator:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        with:
          persist-credentials: false

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y lftp
          pip install --upgrade pip
          pip install selenium

      - name: Create data-aggregator folder
        run: mkdir -p data-aggregator

      - name: Fetch FTP folder structure using ls -R (timeout after 10 minutes)
        env:
          FTP_SERVER: "ftp.ebi.ac.uk"
          FTP_PATH: "/pub/ensemblorganisms"
        run: |
          timeout 10m lftp -c "open $FTP_SERVER; cd $FTP_PATH; ls -R" \
            > data-aggregator/ensembl-beta_ftp_structure.txt || \
          { echo 'lftp command timed out after 10 minutes'; exit 1; }

      - name: Convert structure to JSON (drop only root-file nodes, remove 'test' & 'vep')
        run: |
          python3 << 'EOF'
          import json

          def parse_ls_output(fp):
              tree = {}
              cur = []
              with open(fp) as f:
                  for line in f:
                      line = line.rstrip("\n")
                      if not line:
                          continue
                      if line.endswith(":"):
                          hdr = line[:-1]
                          cur = [] if hdr == "." else hdr.lstrip("./").split("/")
                          continue
                      parts = line.split()
                      if len(parts) < 9:
                          continue
                      name = " ".join(parts[8:])
                      is_dir = parts[0].startswith("d")
                      node = tree
                      for d in cur:
                          node = node.setdefault(d, {})
                      node[name] = {} if is_dir else None
              return tree

          def prune_root_files(tree):
              # remove only top-level file entries (value=None)
              for k, v in list(tree.items()):
                  if v is None:
                      del tree[k]

          def remove_unwanted(tree):
              # remove top-level 'test'
              tree.pop("test", None)
              # remove any 'vep' folder immediately under each genome
              for sp, gm in list(tree.items()):
                  if not isinstance(gm, dict):
                      continue
                  for gid, contents in list(gm.items()):
                      if isinstance(contents, dict):
                          contents.pop("vep", None)
              return tree

          if __name__ == "__main__":
              tree = parse_ls_output("data-aggregator/ensembl-beta_ftp_structure.txt")
              prune_root_files(tree)
              tree = remove_unwanted(tree)
              with open("data-aggregator/ensembl-beta_ftp_structure.json", "w") as out:
                  json.dump(tree, out, indent=4)
              print("Saved JSON with root-file nodes pruned.")
          EOF

      - name: Generate information file
        run: |
          python3 << 'EOF'
          import json

          with open("data-aggregator/ensembl-beta_ftp_structure.json") as f:
              data = json.load(f)

          species_lines = ["Species\tGenome"]
          total_lines   = ["Species\tGenome"]
          sp_count = gen_count = 0

          for species, genomes in data.items():
              if species == "test" or not isinstance(genomes, dict):
                  continue
              count = sum(1 for gid in genomes if gid.startswith(("GCA_","GCF_")))
              species_lines.append(f"{species}\t{count}")
              sp_count  += 1
              gen_count += count

          total_lines.append(f"{sp_count}\t{gen_count}")

          with open("data-aggregator/ensembl-beta_count_species.txt", "w") as sf:
              sf.write("\n".join(species_lines))
          with open("data-aggregator/ensembl-beta_count_total.txt", "w") as tf:
              tf.write("\n".join(total_lines))

          print("Count files saved.")
          EOF

      - name: Fetch SRA advanced index list using Selenium and save as JSON
        run: |
          python3 << 'EOF'
          import time, re, json
          from selenium import webdriver
          from selenium.webdriver.common.by import By
          from selenium.webdriver.support.ui import WebDriverWait, Select
          from selenium.webdriver.support import expected_conditions as EC

          options = webdriver.ChromeOptions()
          options.add_argument("--headless")
          driver = webdriver.Chrome(options=options)
          driver.get("https://www.ncbi.nlm.nih.gov/sra/advanced")
          time.sleep(3)

          fields = [
              ("Strategy", "LS_STRATEGY"),
              ("Source",   "LS_SOURCE"),
              ("Platform", "LS_PLATFORM"),
              ("Selection","LS_SELECTION"),
              ("Filter",   "LS_FILTER"),
              ("Properties","LS_PROPERTIES")
          ]

          results = {}
          for label, var in fields:
              Select(driver.find_element(By.ID, "ff_0")).select_by_visible_text(label)
              time.sleep(1)
              btn = WebDriverWait(driver,10).until(
                  EC.element_to_be_clickable((By.XPATH,"//select[@id='ff_0']/following::a[@class='show_index'][1]"))
              )
              btn.click()
              time.sleep(3)
              opts = driver.find_element(By.ID,"terms_list").find_elements(By.TAG_NAME,"option")
              results[var] = [
                  re.sub(r'\s*\(.*?\)','', opt.text).strip().replace(' ','_')
                  for opt in opts
              ]

          driver.quit()
          with open("data-aggregator/sra_advanced_index.json","w") as out:
              json.dump(results, out, indent=4)
          print("SRA index saved.")
          EOF

      - name: Commit and push changes
        run: |
          git config --global user.name "keun-hong"
          git config --global user.email "thsrms9216@gmail.com"
          if [ -n "$(git status --porcelain data-aggregator/)" ]; then
            git add data-aggregator/
            git commit -m "Update FTP structure, counts & SRA index"
            git push https://snu-cdrc:${{ secrets.MY_PAT }}@github.com/snu-cdrc/gencube.git
          else
            echo "No changes to commit."
          fi
