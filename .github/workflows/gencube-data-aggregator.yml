name: Gencube Data Aggregator Pipeline

on:
  schedule:
    # Runs every 30 minutes (UTC) at 03:00 and 15:00 UTC (i.e. midnight and noon in KST)
    - cron: '0 15 * * *'
  workflow_dispatch:

jobs:
  data-aggregator:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      with:
        persist-credentials: false

    - name: Install dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y lftp
        pip install --upgrade pip
        pip install selenium

    - name: Create data-aggregator folder
      run: mkdir -p data-aggregator

    - name: Fetch FTP folder structure using ls -R
      env:
        FTP_SERVER: "ftp.ebi.ac.uk"  # Replace with your FTP server if needed
        FTP_PATH: "/pub/ensemblorganisms"  # Replace with desired FTP path
      run: |
        lftp -c "open $FTP_SERVER; cd $FTP_PATH; ls -R" > data-aggregator/ensembl-beta_ftp_structure.txt

    - name: Convert structure to JSON (excluding 'vep' folders and the 'test' folder)
      run: |
        python3 << 'EOF'
        import json

        def parse_ls_output(file_path):
            tree = {}
            current_path = []  # Current directory path (list)

            with open(file_path, "r") as f:
                for line in f:
                    line = line.rstrip("\n")
                    if not line:
                        continue
                    # Directory header (lines ending with ':')
                    if line.endswith(":"):
                        header = line[:-1]
                        if header == ".":
                            current_path = []
                        else:
                            # Remove the "./" prefix if present, then split path by "/"
                            if header.startswith("./"):
                                header = header[2:]
                            current_path = header.split("/")
                        continue

                    # ls output: permissions, link count, owner, group, size, date, time, filename.
                    # Join fields from the 9th element onward to accommodate spaces in filenames.
                    parts = line.split()
                    if len(parts) < 9:
                        continue  # Skip lines that do not match expected format
                    filename = " ".join(parts[8:])
                    is_dir = parts[0].startswith("d")

                    # Navigate through the tree based on current_path.
                    node = tree
                    for part in current_path:
                        node = node.setdefault(part, {})
                    if is_dir:
                        node[filename] = {}
                    else:
                        node[filename] = None
            return tree

        def remove_unwanted(tree):
            # Remove the top-level "test" folder if it exists.
            if "test" in tree:
                del tree["test"]
            # For each species (top-level key), remove the "vep" folder from genome folders.
            for species, genomes in tree.items():
                if isinstance(genomes, dict):
                    for genome in list(genomes.keys()):
                        if genome.startswith("GCA_") or genome.startswith("GCF_"):
                            if "vep" in genomes[genome]:
                                del genomes[genome]["vep"]
            return tree

        if __name__ == "__main__":
            input_file = "data-aggregator/ensembl-beta_ftp_structure.txt"  # File generated by lftp
            output_file = "data-aggregator/ensembl-beta_ftp_structure.json"

            directory_tree = parse_ls_output(input_file)
            directory_tree = remove_unwanted(directory_tree)

            with open(output_file, "w") as out_f:
                json.dump(directory_tree, out_f, indent=4)

            print(f"Directory structure saved to {output_file}.")
        EOF

    - name: Generate information file
      run: |
        python3 << 'EOF'
        import json

        # Load the JSON file containing the folder structure.
        with open("data-aggregator/ensembl-beta_ftp_structure.json", "r") as f:
            data = json.load(f)

        # Prepare output tables.
        ls_species = ["Species\tGenome"]
        ls_total = ["Species\tGenome"]
        species_num = 0
        genome_num = 0

        for species, genomes in data.items():
            # Exclude species folder "test" (should already be removed, but extra check)
            if species == "test":
                continue
            count = sum(1 for genome_id in genomes.keys() if genome_id.startswith("GCA_") or genome_id.startswith("GCF_"))
            ls_species.append(f"{species}\t{count}")
            species_num += 1
            genome_num += count

        ls_total.append(f"{species_num}\t{genome_num}")

        with open("data-aggregator/ensembl-beta_count_species.txt", "w") as out_file:
            out_file.write("\n".join(ls_species))
        with open("data-aggregator/ensembl-beta_count_total.txt", "w") as out_file:
            out_file.write("\n".join(ls_total))

        print("Information files saved to data-aggregator/ensembl-beta_count_species.txt and data-aggregator/ensembl-beta_count_total.txt.")
        EOF

    - name: Fetch SRA advanced index list using Selenium and save as JSON
      run: |
        python3 << 'EOF'
        import time, re, json
        from selenium import webdriver
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait, Select
        from selenium.webdriver.support import expected_conditions as EC

        # Set up Chrome options for headless mode
        options = webdriver.ChromeOptions()
        options.add_argument("--headless")

        # Initialize WebDriver (ensure ChromeDriver is installed or available in PATH)
        driver = webdriver.Chrome(options=options)

        # Open the NCBI SRA advanced search page
        driver.get("https://www.ncbi.nlm.nih.gov/sra/advanced")
        time.sleep(3)  # Wait for the page to load

        # List of fields to extract along with the corresponding variable names
        fields_to_extract = [
            ("Strategy", "LS_STRATEGY"),
            ("Source", "LS_SOURCE"),
            ("Platform", "LS_PLATFORM"),
            ("Selection", "LS_SELECTION"),
            ("Filter", "LS_FILTER"),
            ("Properties", "LS_PROPERTIES")
        ]

        results = {}

        # Loop through each field and extract the index list
        for field, varname in fields_to_extract:
            # Select the field from the dropdown (id="ff_0")
            dropdown = Select(driver.find_element(By.ID, "ff_0"))
            dropdown.select_by_visible_text(field)
            time.sleep(1)  # Wait for the selection to be applied

            # Wait until the "Show index list" button is clickable
            show_index_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.XPATH, "//select[@id='ff_0']/following::a[@class='show_index'][1]"))
            )
            # Scroll the button into view
            driver.execute_script("arguments[0].scrollIntoView(true);", show_index_button)
            time.sleep(1)  # Allow time for scrolling
            # Click the button using JavaScript to avoid click interception issues
            driver.execute_script("arguments[0].click();", show_index_button)
            time.sleep(3)  # Wait for the index list to load

            # Retrieve the options from the <select id="terms_list"> element
            terms_select = driver.find_element(By.ID, "terms_list")
            option_elements = terms_select.find_elements(By.TAG_NAME, "option")
            processed_list = []
            for option in option_elements:
                text = option.text.strip()
                # Remove parentheses and its content (e.g., "amplicon (15093690)" -> "amplicon")
                text_no_paren = re.sub(r'\s*\(.*?\)', '', text)
                # Replace spaces with underscores
                processed = text_no_paren.replace(' ', '_')
                processed_list.append(processed)
            results[varname] = processed_list

        driver.quit()

        # Save the results dictionary to a JSON file
        with open("data-aggregator/sra_advanced_index.json", "w") as f:
            json.dump(results, f, indent=4)

        print("SRA advanced index list saved to data-aggregator/sra_advanced_index.json.")
        EOF

    - name: Commit and push changes
      run: |
        git config --global user.name "keun-hong"
        git config --global user.email "thsrms9216@gmail.com"
        if [ -n "$(git status --porcelain data-aggregator/ensembl-beta_ftp_structure.json data-aggregator/ensembl-beta_count_species.txt data-aggregator/ensembl-beta_count_total.txt data-aggregator/sra_advanced_index.json)" ]; then
          git add data-aggregator/ensembl-beta_ftp_structure.json data-aggregator/ensembl-beta_count_species.txt data-aggregator/ensembl-beta_count_total.txt data-aggregator/sra_advanced_index.json
          git commit -m "Update FTP folder structure, information, and SRA advanced index list"
          git push https://snu-cdrc:${{ secrets.MY_PAT }}@github.com/snu-cdrc/gencube.git
        else
          echo "No changes to commit."
        fi
